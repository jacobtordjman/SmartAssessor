{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 1800,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.03333333333333333,
      "grad_norm": 23.701534271240234,
      "learning_rate": 0.00019844444444444445,
      "loss": 3.4557,
      "step": 20
    },
    {
      "epoch": 0.06666666666666667,
      "grad_norm": 2.0689830780029297,
      "learning_rate": 0.00019644444444444445,
      "loss": 0.3927,
      "step": 40
    },
    {
      "epoch": 0.1,
      "grad_norm": 1.875282883644104,
      "learning_rate": 0.00019422222222222223,
      "loss": 0.103,
      "step": 60
    },
    {
      "epoch": 0.13333333333333333,
      "grad_norm": 13.988688468933105,
      "learning_rate": 0.000192,
      "loss": 0.0826,
      "step": 80
    },
    {
      "epoch": 0.16666666666666666,
      "grad_norm": 0.9130686521530151,
      "learning_rate": 0.00018977777777777778,
      "loss": 0.072,
      "step": 100
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.11162472516298294,
      "learning_rate": 0.00018755555555555558,
      "loss": 0.0617,
      "step": 120
    },
    {
      "epoch": 0.23333333333333334,
      "grad_norm": 0.09449176490306854,
      "learning_rate": 0.00018533333333333333,
      "loss": 0.0621,
      "step": 140
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 1.2771971225738525,
      "learning_rate": 0.00018311111111111113,
      "loss": 0.0706,
      "step": 160
    },
    {
      "epoch": 0.3,
      "grad_norm": 0.4859714210033417,
      "learning_rate": 0.000181,
      "loss": 0.1041,
      "step": 180
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 0.1756105273962021,
      "learning_rate": 0.00017877777777777778,
      "loss": 0.0654,
      "step": 200
    },
    {
      "epoch": 0.36666666666666664,
      "grad_norm": 0.084520623087883,
      "learning_rate": 0.00017655555555555556,
      "loss": 0.0586,
      "step": 220
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.3257814943790436,
      "learning_rate": 0.00017433333333333336,
      "loss": 0.054,
      "step": 240
    },
    {
      "epoch": 0.43333333333333335,
      "grad_norm": 4.908791542053223,
      "learning_rate": 0.0001721111111111111,
      "loss": 0.0488,
      "step": 260
    },
    {
      "epoch": 0.4666666666666667,
      "grad_norm": 0.32050031423568726,
      "learning_rate": 0.0001698888888888889,
      "loss": 0.0378,
      "step": 280
    },
    {
      "epoch": 0.5,
      "grad_norm": 2.387720823287964,
      "learning_rate": 0.00016766666666666669,
      "loss": 0.0473,
      "step": 300
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 0.07635314017534256,
      "learning_rate": 0.00016544444444444443,
      "loss": 0.0496,
      "step": 320
    },
    {
      "epoch": 0.5666666666666667,
      "grad_norm": 0.1019408330321312,
      "learning_rate": 0.00016322222222222224,
      "loss": 0.0437,
      "step": 340
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.32650843262672424,
      "learning_rate": 0.000161,
      "loss": 0.044,
      "step": 360
    },
    {
      "epoch": 0.6333333333333333,
      "grad_norm": 0.2797714173793793,
      "learning_rate": 0.00015877777777777779,
      "loss": 0.0508,
      "step": 380
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 1.5961581468582153,
      "learning_rate": 0.00015666666666666666,
      "loss": 0.0812,
      "step": 400
    },
    {
      "epoch": 0.7,
      "grad_norm": 0.0600164458155632,
      "learning_rate": 0.00015444444444444446,
      "loss": 0.039,
      "step": 420
    },
    {
      "epoch": 0.7333333333333333,
      "grad_norm": 0.07023128122091293,
      "learning_rate": 0.0001522222222222222,
      "loss": 0.0354,
      "step": 440
    },
    {
      "epoch": 0.7666666666666667,
      "grad_norm": 0.24921686947345734,
      "learning_rate": 0.00015000000000000001,
      "loss": 0.0427,
      "step": 460
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.7428468465805054,
      "learning_rate": 0.0001477777777777778,
      "loss": 0.0353,
      "step": 480
    },
    {
      "epoch": 0.8333333333333334,
      "grad_norm": 0.7797304391860962,
      "learning_rate": 0.00014555555555555556,
      "loss": 0.038,
      "step": 500
    },
    {
      "epoch": 0.8666666666666667,
      "grad_norm": 0.33310428261756897,
      "learning_rate": 0.00014333333333333334,
      "loss": 0.0453,
      "step": 520
    },
    {
      "epoch": 0.9,
      "grad_norm": 0.7384219765663147,
      "learning_rate": 0.00014111111111111111,
      "loss": 0.0425,
      "step": 540
    },
    {
      "epoch": 0.9333333333333333,
      "grad_norm": 0.3161768615245819,
      "learning_rate": 0.0001388888888888889,
      "loss": 0.0378,
      "step": 560
    },
    {
      "epoch": 0.9666666666666667,
      "grad_norm": 0.08136949688196182,
      "learning_rate": 0.00013666666666666666,
      "loss": 0.0373,
      "step": 580
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.10166371613740921,
      "learning_rate": 0.00013444444444444447,
      "loss": 0.0414,
      "step": 600
    },
    {
      "epoch": 1.0333333333333334,
      "grad_norm": 0.15502329170703888,
      "learning_rate": 0.00013222222222222221,
      "loss": 0.0434,
      "step": 620
    },
    {
      "epoch": 1.0666666666666667,
      "grad_norm": 0.0793963074684143,
      "learning_rate": 0.00013000000000000002,
      "loss": 0.0377,
      "step": 640
    },
    {
      "epoch": 1.1,
      "grad_norm": 0.08814738690853119,
      "learning_rate": 0.00012777777777777776,
      "loss": 0.0383,
      "step": 660
    },
    {
      "epoch": 1.1333333333333333,
      "grad_norm": 0.4487815797328949,
      "learning_rate": 0.00012555555555555557,
      "loss": 0.0391,
      "step": 680
    },
    {
      "epoch": 1.1666666666666667,
      "grad_norm": 0.3184981942176819,
      "learning_rate": 0.00012333333333333334,
      "loss": 0.0436,
      "step": 700
    },
    {
      "epoch": 1.2,
      "grad_norm": 0.17009155452251434,
      "learning_rate": 0.0001211111111111111,
      "loss": 0.0409,
      "step": 720
    },
    {
      "epoch": 1.2333333333333334,
      "grad_norm": 0.6792364120483398,
      "learning_rate": 0.00011888888888888889,
      "loss": 0.0369,
      "step": 740
    },
    {
      "epoch": 1.2666666666666666,
      "grad_norm": 1.408792495727539,
      "learning_rate": 0.00011666666666666668,
      "loss": 0.0361,
      "step": 760
    },
    {
      "epoch": 1.3,
      "grad_norm": 0.1215076893568039,
      "learning_rate": 0.00011444444444444444,
      "loss": 0.0385,
      "step": 780
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 0.6638580560684204,
      "learning_rate": 0.00011222222222222223,
      "loss": 0.0374,
      "step": 800
    },
    {
      "epoch": 1.3666666666666667,
      "grad_norm": 0.16671842336654663,
      "learning_rate": 0.00011000000000000002,
      "loss": 0.0359,
      "step": 820
    },
    {
      "epoch": 1.4,
      "grad_norm": 0.6443701982498169,
      "learning_rate": 0.00010777777777777778,
      "loss": 0.0374,
      "step": 840
    },
    {
      "epoch": 1.4333333333333333,
      "grad_norm": 0.5203009843826294,
      "learning_rate": 0.00010555555555555557,
      "loss": 0.038,
      "step": 860
    },
    {
      "epoch": 1.4666666666666668,
      "grad_norm": 0.37119895219802856,
      "learning_rate": 0.00010333333333333334,
      "loss": 0.0329,
      "step": 880
    },
    {
      "epoch": 1.5,
      "grad_norm": 0.11491068452596664,
      "learning_rate": 0.00010111111111111112,
      "loss": 0.0381,
      "step": 900
    },
    {
      "epoch": 1.5333333333333332,
      "grad_norm": 0.33901840448379517,
      "learning_rate": 9.888888888888889e-05,
      "loss": 0.0318,
      "step": 920
    },
    {
      "epoch": 1.5666666666666667,
      "grad_norm": 0.5774909853935242,
      "learning_rate": 9.666666666666667e-05,
      "loss": 0.0365,
      "step": 940
    },
    {
      "epoch": 1.6,
      "grad_norm": 0.15498223900794983,
      "learning_rate": 9.444444444444444e-05,
      "loss": 0.039,
      "step": 960
    },
    {
      "epoch": 1.6333333333333333,
      "grad_norm": 0.22889955341815948,
      "learning_rate": 9.222222222222223e-05,
      "loss": 0.0389,
      "step": 980
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 0.2761904299259186,
      "learning_rate": 9e-05,
      "loss": 0.0336,
      "step": 1000
    },
    {
      "epoch": 1.7,
      "grad_norm": 0.12489067763090134,
      "learning_rate": 8.777777777777778e-05,
      "loss": 0.0425,
      "step": 1020
    },
    {
      "epoch": 1.7333333333333334,
      "grad_norm": 0.6727485060691833,
      "learning_rate": 8.555555555555556e-05,
      "loss": 0.0322,
      "step": 1040
    },
    {
      "epoch": 1.7666666666666666,
      "grad_norm": 0.10203439742326736,
      "learning_rate": 8.333333333333334e-05,
      "loss": 0.0359,
      "step": 1060
    },
    {
      "epoch": 1.8,
      "grad_norm": 0.15758806467056274,
      "learning_rate": 8.111111111111112e-05,
      "loss": 0.0333,
      "step": 1080
    },
    {
      "epoch": 1.8333333333333335,
      "grad_norm": 0.10362546890974045,
      "learning_rate": 7.88888888888889e-05,
      "loss": 0.0294,
      "step": 1100
    },
    {
      "epoch": 1.8666666666666667,
      "grad_norm": 0.10178517550230026,
      "learning_rate": 7.666666666666667e-05,
      "loss": 0.033,
      "step": 1120
    },
    {
      "epoch": 1.9,
      "grad_norm": 0.1298718899488449,
      "learning_rate": 7.444444444444444e-05,
      "loss": 0.0299,
      "step": 1140
    },
    {
      "epoch": 1.9333333333333333,
      "grad_norm": 0.18149226903915405,
      "learning_rate": 7.222222222222222e-05,
      "loss": 0.0359,
      "step": 1160
    },
    {
      "epoch": 1.9666666666666668,
      "grad_norm": 0.22124426066875458,
      "learning_rate": 7e-05,
      "loss": 0.0322,
      "step": 1180
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.1728859543800354,
      "learning_rate": 6.777777777777778e-05,
      "loss": 0.0329,
      "step": 1200
    },
    {
      "epoch": 2.033333333333333,
      "grad_norm": 0.3060210347175598,
      "learning_rate": 6.555555555555556e-05,
      "loss": 0.0329,
      "step": 1220
    },
    {
      "epoch": 2.066666666666667,
      "grad_norm": 0.2919590175151825,
      "learning_rate": 6.333333333333333e-05,
      "loss": 0.0254,
      "step": 1240
    },
    {
      "epoch": 2.1,
      "grad_norm": 0.14929404854774475,
      "learning_rate": 6.111111111111112e-05,
      "loss": 0.0315,
      "step": 1260
    },
    {
      "epoch": 2.1333333333333333,
      "grad_norm": 0.16379599273204803,
      "learning_rate": 5.8888888888888896e-05,
      "loss": 0.0321,
      "step": 1280
    },
    {
      "epoch": 2.1666666666666665,
      "grad_norm": 0.3419782221317291,
      "learning_rate": 5.666666666666667e-05,
      "loss": 0.0287,
      "step": 1300
    },
    {
      "epoch": 2.2,
      "grad_norm": 0.22163160145282745,
      "learning_rate": 5.4444444444444446e-05,
      "loss": 0.0397,
      "step": 1320
    },
    {
      "epoch": 2.2333333333333334,
      "grad_norm": 0.2525171637535095,
      "learning_rate": 5.222222222222223e-05,
      "loss": 0.0326,
      "step": 1340
    },
    {
      "epoch": 2.2666666666666666,
      "grad_norm": 0.4927372634410858,
      "learning_rate": 5e-05,
      "loss": 0.0443,
      "step": 1360
    },
    {
      "epoch": 2.3,
      "grad_norm": 0.3627541661262512,
      "learning_rate": 4.7777777777777784e-05,
      "loss": 0.0308,
      "step": 1380
    },
    {
      "epoch": 2.3333333333333335,
      "grad_norm": 0.38881421089172363,
      "learning_rate": 4.555555555555556e-05,
      "loss": 0.0301,
      "step": 1400
    },
    {
      "epoch": 2.3666666666666667,
      "grad_norm": 0.27840396761894226,
      "learning_rate": 4.3333333333333334e-05,
      "loss": 0.0304,
      "step": 1420
    },
    {
      "epoch": 2.4,
      "grad_norm": 0.6116984486579895,
      "learning_rate": 4.111111111111111e-05,
      "loss": 0.0273,
      "step": 1440
    },
    {
      "epoch": 2.4333333333333336,
      "grad_norm": 0.5034841895103455,
      "learning_rate": 3.888888888888889e-05,
      "loss": 0.0295,
      "step": 1460
    },
    {
      "epoch": 2.466666666666667,
      "grad_norm": 0.14216256141662598,
      "learning_rate": 3.6666666666666666e-05,
      "loss": 0.0331,
      "step": 1480
    },
    {
      "epoch": 2.5,
      "grad_norm": 0.17984536290168762,
      "learning_rate": 3.444444444444445e-05,
      "loss": 0.0242,
      "step": 1500
    },
    {
      "epoch": 2.533333333333333,
      "grad_norm": 0.5895327925682068,
      "learning_rate": 3.222222222222223e-05,
      "loss": 0.0277,
      "step": 1520
    },
    {
      "epoch": 2.5666666666666664,
      "grad_norm": 0.616436779499054,
      "learning_rate": 3e-05,
      "loss": 0.0272,
      "step": 1540
    },
    {
      "epoch": 2.6,
      "grad_norm": 0.476367712020874,
      "learning_rate": 2.777777777777778e-05,
      "loss": 0.0254,
      "step": 1560
    },
    {
      "epoch": 2.6333333333333333,
      "grad_norm": 0.39681559801101685,
      "learning_rate": 2.5555555555555554e-05,
      "loss": 0.0268,
      "step": 1580
    },
    {
      "epoch": 2.6666666666666665,
      "grad_norm": 0.31319642066955566,
      "learning_rate": 2.3333333333333336e-05,
      "loss": 0.0263,
      "step": 1600
    },
    {
      "epoch": 2.7,
      "grad_norm": 0.3845786154270172,
      "learning_rate": 2.111111111111111e-05,
      "loss": 0.0251,
      "step": 1620
    },
    {
      "epoch": 2.7333333333333334,
      "grad_norm": 0.9249667525291443,
      "learning_rate": 1.888888888888889e-05,
      "loss": 0.0232,
      "step": 1640
    },
    {
      "epoch": 2.7666666666666666,
      "grad_norm": 0.23741289973258972,
      "learning_rate": 1.6666666666666667e-05,
      "loss": 0.0283,
      "step": 1660
    },
    {
      "epoch": 2.8,
      "grad_norm": 0.4433199167251587,
      "learning_rate": 1.4444444444444444e-05,
      "loss": 0.0248,
      "step": 1680
    },
    {
      "epoch": 2.8333333333333335,
      "grad_norm": 0.2368450164794922,
      "learning_rate": 1.2222222222222222e-05,
      "loss": 0.0208,
      "step": 1700
    },
    {
      "epoch": 2.8666666666666667,
      "grad_norm": 0.7710702419281006,
      "learning_rate": 1e-05,
      "loss": 0.023,
      "step": 1720
    },
    {
      "epoch": 2.9,
      "grad_norm": 0.5360130071640015,
      "learning_rate": 7.777777777777777e-06,
      "loss": 0.0231,
      "step": 1740
    },
    {
      "epoch": 2.9333333333333336,
      "grad_norm": 0.4022217392921448,
      "learning_rate": 5.555555555555556e-06,
      "loss": 0.0225,
      "step": 1760
    },
    {
      "epoch": 2.966666666666667,
      "grad_norm": 0.24100951850414276,
      "learning_rate": 3.3333333333333333e-06,
      "loss": 0.0245,
      "step": 1780
    },
    {
      "epoch": 3.0,
      "grad_norm": 0.49429404735565186,
      "learning_rate": 1.1111111111111112e-06,
      "loss": 0.0244,
      "step": 1800
    }
  ],
  "logging_steps": 20,
  "max_steps": 1800,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 200,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 3.9356794404864e+16,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
